{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap train at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Explain concepts of incremental fit by chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/train_by_chunk.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Explain code solution for `main_local.train()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Training on the full (already preprocessed) dataset, by loading it\n",
    "    chunk-by-chunk, and updating the weight of the model for each chunks.\n",
    "    Save model, compute validation metrics on a holdout validation set that is\n",
    "    common to all chunks.\n",
    "    \"\"\"\n",
    "    print(\"\\n ‚≠êÔ∏è use case: train\")\n",
    "\n",
    "    # Validation Set: Load a validation set common to all chunks and create X_val, y_val\n",
    "    data_val_processed_path = os.path.abspath(os.path.join(\n",
    "        LOCAL_DATA_PATH, \"processed\", f\"val_processed_{VALIDATION_DATASET_SIZE}.csv\"))\n",
    "\n",
    "    \n",
    "    data_val_processed = pd.read_csv(\n",
    "        data_val_processed_path,\n",
    "        header=None,\n",
    "        dtype=DTYPES_PROCESSED_OPTIMIZED\n",
    "        ).to_numpy()\n",
    "\n",
    "    X_val = data_val_processed[:, :-1]\n",
    "    y_val = data_val_processed[:, -1]\n",
    "    \n",
    "\n",
    "    # Iterate on the full training dataset chunk per chunks.\n",
    "    # Break out of the loop if you receive no more data to train upon!\n",
    "    model = None\n",
    "    chunk_id = 0\n",
    "    metrics_val_list = []  # store each metrics_val_chunk\n",
    "\n",
    "    while (True):\n",
    "        print(f\"loading and training on preprocessed chunk n¬∞{chunk_id}...\")\n",
    "\n",
    "        # Load chunk of preprocess data and create (X_train_chunk, y_train_chunk)\n",
    "        \n",
    "        path = os.path.abspath(os.path.join(\n",
    "            LOCAL_DATA_PATH, \"processed\", f\"train_processed_{DATASET_SIZE}.csv\"))\n",
    "\n",
    "        try:\n",
    "            data_processed_chunk = pd.read_csv(\n",
    "                    path,\n",
    "                    header=None,\n",
    "                    skiprows=(chunk_id * CHUNK_SIZE),\n",
    "                    nrows=CHUNK_SIZE,\n",
    "                    dtype=DTYPES_PROCESSED_OPTIMIZED,\n",
    "                    ).to_numpy()\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            data_processed_chunk = None  # end of data\n",
    "\n",
    "        # Break out of while loop if we have no data to train upon\n",
    "        if data_processed_chunk is None:\n",
    "            break\n",
    "\n",
    "        X_train_chunk = data_processed_chunk[:, :-1]\n",
    "        y_train_chunk = data_processed_chunk[:, -1]\n",
    "        \n",
    "\n",
    "        # Train a model incrementally and print validation metrics for this chunk\n",
    "        learning_rate = 0.001\n",
    "        batch_size = 256\n",
    "        \n",
    "        if model is None:\n",
    "            model = initialize_model(X_train_chunk)\n",
    "            model = compile_model(model, learning_rate)\n",
    "\n",
    "        model, history = train_model(model,\n",
    "                                     X_train_chunk,\n",
    "                                     y_train_chunk,\n",
    "                                     batch_size,\n",
    "                                     validation_data=(X_val, y_val))\n",
    "        metrics_val_chunk = np.min(history.history['val_mae'])\n",
    "        metrics_val_list.append(metrics_val_chunk)\n",
    "        print(metrics_val_chunk)\n",
    "        \n",
    "\n",
    "        chunk_id += 1\n",
    "\n",
    "    # Save model and training params\n",
    "    params = dict(\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        incremental=True,\n",
    "        chunk_size=CHUNK_SIZE)\n",
    "\n",
    "    metrics_val_mean_all_chunks = np.mean(np.array(metrics_val_list))\n",
    "    metrics = dict(mean_val=metrics_val_mean_all_chunks)\n",
    "\n",
    "    save_model(model, params=params, metrics=metrics)\n",
    "\n",
    "    print(\"‚úÖ model trained and saved\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) üíª Tensorflow tricks to partial fit without manual chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**üìöResourcesüìö**\n",
    "- tf CSV guide: https://www.tensorflow.org/guide/data#consuming_csv_data\n",
    "- tf CSV tuto: https://www.tensorflow.org/tutorials/load_data/csv\n",
    "- tf Datasets https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb#scrollTo=x5z5B11UjDTd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/juan-garassino/code/juan-garassino/MLops-taxiFare/taxifare/__init__.py'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import taxifare\n",
    "deepCab.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepCab.ml_logic.params import LOCAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_path_small = \"/home/juan-garassino/code/juan-garassino/MLops-taxiFare/taxifare/data/processed/train_processed_10k.csv\"#os.path.join(LOCAL_DATA_PATH, \"processed\",\"train_processed_10K.csv\")\n",
    "#data_processed_path = os.path.join(LOCAL_DATA_PATH, \"processed\",\"train_processed_500K.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/juan-garassino/code/juan-garassino/MLops-taxiFare/taxifare/data/processed/train_processed_10k.csv'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed_path_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy paste it below to make it more explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    reg = regularizers.l1_l2(l2=0.005)\n",
    "    model = Sequential()\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(50, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.99))  # use momentum=0 for to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate= 0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\",\n",
    "                       patience=2,\n",
    "                       restore_best_weights=True,\n",
    "                       verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) If data fit in memory üòá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.299999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9  ...   56   57   58  \\\n",
       "0  0.142857  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.142857  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.000000  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    59   60   61   62   63   64         65  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  41.299999  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  10.000000  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0   5.000000  \n",
       "\n",
       "[3 rows x 66 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = pd.read_csv(data_processed_path_small)\n",
    "df_small.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_small.drop(columns=['65']).to_numpy()\n",
    "target = df_small[['65']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9765, 65)\n",
      "(9765, 1)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(target.shape)\n",
    "n_samples = features.shape[0]\n",
    "n_features = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) passing numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26/26 [==============================] - 7s 66ms/step - loss: 229.2076 - mae: 11.3499 - val_loss: 225.2875 - val_mae: 11.2278\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 210.5648 - mae: 11.2120 - val_loss: 222.4642 - val_mae: 11.2155\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 198.7945 - mae: 11.0666 - val_loss: 221.9992 - val_mae: 11.3542\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 188.3025 - mae: 10.8929 - val_loss: 221.9827 - val_mae: 11.5474\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 177.6878 - mae: 10.6908 - val_loss: 228.6076 - val_mae: 12.0478\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 167.5652 - mae: 10.4805 - val_loss: 238.0772 - val_mae: 12.6360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7345d9efd0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.fit(x=features, y=target, batch_size=BATCH_SIZE, validation_split=0.3, callbacks=[es], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) passing `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((features, target))\n",
    "ds = ds.batch(BATCH_SIZE)  # Set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 65), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:35:09.069752: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([265, 65]), TensorShape([265, 1]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First sample: feature_1, target_1\n",
    "f1, t1 = next(iter(ds))\n",
    "(f1.shape, t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "37/37 [==============================] - 7s 22ms/step - loss: 222.3228 - mae: 11.2756\n",
      "Epoch 2/5\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 201.6166 - mae: 11.1128\n",
      "Epoch 3/5\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 186.7283 - mae: 10.9225\n",
      "Epoch 4/5\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 171.0855 - mae: 10.6558\n",
      "Epoch 5/5\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 156.2453 - mae: 10.3661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7346b11700>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) If data is too large to fit in memory ? üßê "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Use `make_csv_dataset` helper\n",
    "\n",
    "More info on this tutorial https://www.tensorflow.org/tutorials/load_data/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.make_csv_dataset(\n",
    "    data_processed_path_small, #data_processed_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    header=False,\n",
    "    column_names=list(df_small.columns),\n",
    "    label_name='65',\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('0', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('1', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('2', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('3', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('4', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('5', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('6', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('7', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('8', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('9', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('10', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('11', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('12', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('13', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('14', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('15', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('16', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('17', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('18', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('19', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('20', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('21', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('22', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('23', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('24', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('25', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('26', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('27', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('28', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('29', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('30', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('31', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('32', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('33', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('34', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('35', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('36', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('37', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('38', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('39', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('40', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('41', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('42', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('43', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('44', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('45', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('46', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('47', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('48', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('49', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('50', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('51', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('52', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('53', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('54', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('55', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('56', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('57', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('58', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('59', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('60', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('61', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('62', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('63', TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       "              ('64', TensorSpec(shape=(None,), dtype=tf.float32, name=None))]),\n",
       " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1, target1 = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(265,), dtype=float32, numpy=\n",
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.14285715, 0.14285715, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5714286 , 0.        , 0.        , 0.14285715,\n",
       "       0.5714286 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.2857143 , 0.71428573, 0.        , 0.2857143 ,\n",
       "       0.14285715, 0.        , 0.        , 0.2857143 , 0.        ,\n",
       "       0.2857143 , 0.2857143 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.14285715, 0.5714286 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.        , 0.        , 0.        ,\n",
       "       0.14285715, 0.        , 0.        , 0.        , 0.5714286 ,\n",
       "       0.        , 0.        , 0.        , 0.14285715, 0.14285715,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.2857143 ,\n",
       "       0.        , 0.        , 0.        , 0.71428573, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.2857143 , 0.        , 0.        ,\n",
       "       0.14285715, 0.71428573, 0.5714286 , 0.        , 0.14285715,\n",
       "       0.        , 0.        , 0.        , 0.14285715, 0.        ,\n",
       "       0.5714286 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.        , 0.14285715, 0.        , 0.2857143 , 0.        ,\n",
       "       0.        , 0.71428573, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.        , 0.5714286 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.5714286 , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.5714286 ,\n",
       "       0.        , 0.        , 0.        , 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.5714286 , 0.        , 0.14285715,\n",
       "       0.5714286 , 0.14285715, 0.14285715, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.14285715, 0.        , 0.        ,\n",
       "       0.2857143 , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.14285715, 0.14285715, 0.        , 0.        , 0.14285715,\n",
       "       0.5714286 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.5714286 , 0.        , 0.5714286 , 0.        , 0.14285715,\n",
       "       0.        , 0.42857143, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5714286 , 0.2857143 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.        , 0.5714286 , 0.        , 0.        , 0.        ,\n",
       "       0.14285715, 0.        , 0.5714286 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.5714286 , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.2857143 , 0.14285715, 0.42857143, 0.5714286 ,\n",
       "       0.        , 0.14285715, 0.5714286 , 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.2857143 , 0.        , 0.14285715],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(265,), dtype=float32, numpy=\n",
       "array([0.        , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.14285715, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.5714286 , 0.        , 0.42857143, 0.5714286 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.14285715, 0.        ,\n",
       "       0.71428573, 0.14285715, 0.        , 0.        , 0.14285715,\n",
       "       0.        , 0.5714286 , 0.14285715, 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.5714286 , 0.        ,\n",
       "       0.2857143 , 0.        , 0.5714286 , 0.        , 0.        ,\n",
       "       0.42857143, 0.        , 0.        , 0.14285715, 0.14285715,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.14285715, 0.2857143 , 0.2857143 , 0.        , 0.        ,\n",
       "       0.14285715, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.2857143 , 0.        , 0.        , 0.        , 0.5714286 ,\n",
       "       0.        , 0.14285715, 0.14285715, 0.2857143 , 0.14285715,\n",
       "       0.        , 0.        , 0.5714286 , 0.        , 0.14285715,\n",
       "       0.5714286 , 0.        , 0.14285715, 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2857143 , 0.        ,\n",
       "       0.        , 0.        , 0.5714286 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.42857143, 0.        , 0.5714286 , 0.        , 0.        ,\n",
       "       0.42857143, 0.        , 0.71428573, 0.14285715, 0.        ,\n",
       "       0.        , 0.5714286 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.2857143 , 0.        , 0.        ,\n",
       "       0.14285715, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.        , 0.        , 0.14285715,\n",
       "       0.        , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.14285715, 0.14285715, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14285715, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.71428573, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14285715, 0.5714286 , 0.5714286 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14285715, 0.        , 0.        , 0.14285715,\n",
       "       0.        , 0.14285715, 0.        , 0.14285715, 0.5714286 ,\n",
       "       0.14285715, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.14285715, 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.42857143, 0.        ,\n",
       "       0.71428573, 0.        , 0.        , 0.71428573, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.71428573,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.42857143, 0.2857143 , 0.42857143, 0.        , 0.14285715,\n",
       "       0.2857143 , 0.5714286 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.14285715,\n",
       "       0.2857143 , 0.        , 0.5714286 , 0.14285715, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.42857143, 0.        ,\n",
       "       0.5714286 , 0.14285715, 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(265,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(265,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1['64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([265, 65])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stack(x):\n",
    "    return tf.stack([x[f'{i}'] for i in range(65)], axis=1)\n",
    "\n",
    "stack(feat1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda x,y: (stack(x),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=stack(feat1)\n",
    "\n",
    "y=target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 6s 14ms/step - loss: 240.4458 - mae: 11.4696\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 228.0678 - mae: 11.4249\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 218.8458 - mae: 11.3691\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 211.2357 - mae: 11.3342\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 205.7926 - mae: 11.2986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73bbc4bdf0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(x,y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('taxi-fare')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "34b6111846e55500eb2d1311eae5fc193806a80dd8c2ff2785838739b774f835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
