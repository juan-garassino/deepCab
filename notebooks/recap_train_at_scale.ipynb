{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap train at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Explain concepts of incremental fit by chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/train_by_chunk.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Explain code solution for `main_local.train()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Training on the full (already preprocessed) dataset, by loading it\n",
    "    chunk-by-chunk, and updating the weight of the model for each chunks.\n",
    "    Save model, compute validation metrics on a holdout validation set that is\n",
    "    common to all chunks.\n",
    "    \"\"\"\n",
    "    print(\"\\n ‚≠êÔ∏è use case: train\")\n",
    "\n",
    "    # Validation Set: Load a validation set common to all chunks and create X_val, y_val\n",
    "    data_val_processed_path = os.path.abspath(os.path.join(\n",
    "        LOCAL_DATA_PATH, \"processed\", f\"val_processed_{VALIDATION_DATASET_SIZE}.csv\"))\n",
    "\n",
    "    \n",
    "    data_val_processed = pd.read_csv(\n",
    "        data_val_processed_path,\n",
    "        header=None,\n",
    "        dtype=DTYPES_PROCESSED_OPTIMIZED\n",
    "        ).to_numpy()\n",
    "\n",
    "    X_val = data_val_processed[:, :-1]\n",
    "    y_val = data_val_processed[:, -1]\n",
    "    \n",
    "\n",
    "    # Iterate on the full training dataset chunk per chunks.\n",
    "    # Break out of the loop if you receive no more data to train upon!\n",
    "    model = None\n",
    "    chunk_id = 0\n",
    "    metrics_val_list = []  # store each metrics_val_chunk\n",
    "\n",
    "    while (True):\n",
    "        print(f\"loading and training on preprocessed chunk n¬∞{chunk_id}...\")\n",
    "\n",
    "        # Load chunk of preprocess data and create (X_train_chunk, y_train_chunk)\n",
    "        \n",
    "        path = os.path.abspath(os.path.join(\n",
    "            LOCAL_DATA_PATH, \"processed\", f\"train_processed_{DATASET_SIZE}.csv\"))\n",
    "\n",
    "        try:\n",
    "            data_processed_chunk = pd.read_csv(\n",
    "                    path,\n",
    "                    header=None,\n",
    "                    skiprows=(chunk_id * CHUNK_SIZE),\n",
    "                    nrows=CHUNK_SIZE,\n",
    "                    dtype=DTYPES_PROCESSED_OPTIMIZED,\n",
    "                    ).to_numpy()\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            data_processed_chunk = None  # end of data\n",
    "\n",
    "        # Break out of while loop if we have no data to train upon\n",
    "        if data_processed_chunk is None:\n",
    "            break\n",
    "\n",
    "        X_train_chunk = data_processed_chunk[:, :-1]\n",
    "        y_train_chunk = data_processed_chunk[:, -1]\n",
    "        \n",
    "\n",
    "        # Train a model incrementally and print validation metrics for this chunk\n",
    "        learning_rate = 0.001\n",
    "        batch_size = 256\n",
    "        \n",
    "        if model is None:\n",
    "            model = initialize_model(X_train_chunk)\n",
    "            model = compile_model(model, learning_rate)\n",
    "\n",
    "        model, history = train_model(model,\n",
    "                                     X_train_chunk,\n",
    "                                     y_train_chunk,\n",
    "                                     batch_size,\n",
    "                                     validation_data=(X_val, y_val))\n",
    "        metrics_val_chunk = np.min(history.history['val_mae'])\n",
    "        metrics_val_list.append(metrics_val_chunk)\n",
    "        print(metrics_val_chunk)\n",
    "        \n",
    "\n",
    "        chunk_id += 1\n",
    "\n",
    "    # Save model and training params\n",
    "    params = dict(\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        incremental=True,\n",
    "        chunk_size=CHUNK_SIZE)\n",
    "\n",
    "    metrics_val_mean_all_chunks = np.mean(np.array(metrics_val_list))\n",
    "    metrics = dict(mean_val=metrics_val_mean_all_chunks)\n",
    "\n",
    "    save_model(model, params=params, metrics=metrics)\n",
    "\n",
    "    print(\"‚úÖ model trained and saved\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) üíª Tensorflow tricks to partial fit without manual chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**üìöResourcesüìö**\n",
    "- tf CSV guide: https://www.tensorflow.org/guide/data#consuming_csv_data\n",
    "- tf CSV tuto: https://www.tensorflow.org/tutorials/load_data/csv\n",
    "- tf Datasets https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb#scrollTo=x5z5B11UjDTd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import taxifare\n",
    "taxifare.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxifare.ml_logic.params import LOCAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_path_small = os.path.join(LOCAL_DATA_PATH, \"processed\",\"train_processed_100K.csv\")\n",
    "data_processed_path = os.path.join(LOCAL_DATA_PATH, \"processed\",\"train_processed_500K.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy paste it below to make it more explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    reg = regularizers.l1_l2(l2=0.005)\n",
    "    model = Sequential()\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(50, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.99))  # use momentum=0 for to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate= 0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\",\n",
    "                       patience=2,\n",
    "                       restore_best_weights=True,\n",
    "                       verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) If data fit in memory üòá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = pd.read_csv(data_processed_path_small)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_small.drop(columns=['65']).to_numpy()\n",
    "target = df_small[['65']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(target.shape)\n",
    "n_samples = features.shape[0]\n",
    "n_features = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) passing numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.fit(x=features, y=target, batch_size=BATCH_SIZE, validation_split=0.3, callbacks=[es], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) passing `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((features, target))\n",
    "ds = ds.batch(BATCH_SIZE)  # Set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sample: feature_1, target_1\n",
    "f1, t1 = next(iter(ds))\n",
    "(f1.shape, t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) If data is too large to fit in memory ? üßê "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Use `make_csv_dataset` helper\n",
    "\n",
    "More info on this tutorial https://www.tensorflow.org/tutorials/load_data/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.make_csv_dataset(\n",
    "    data_processed_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    header=False,\n",
    "    column_names=list(df_small.columns),\n",
    "    label_name='65',\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1, target1 = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(x):\n",
    "    return tf.stack([x[f'{i}'] for i in range(65)], axis=1)\n",
    "\n",
    "stack(feat1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda x,y: (stack(x),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(ds, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
